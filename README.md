# DAVoteNet-release
Official implementation for paper: Investigating Domain Gaps for Indoor 3D Object Detection

As a fundamental task for indoor scene understanding, 3D object detection has been extensively studied, and the accuracy on indoor point cloud data has been substantially improved. 
However, existing researches have been conducted on limited datasets, where the training and testing sets share the same distribution. 
In this paper, we consider the task of adapting indoor 3D object detectors from one dataset to another, presenting a comprehensive benchmark with ScanNet, SUN RGB-D and 3D Front datasets, as well as our newly proposed large-scale datasets ProcTHOR-OD and ProcFront generated by a 3D simulator. 
Since indoor point cloud datasets are collected and constructed in different ways, the object detectors are likely to overfit to specific factors within each dataset, such as point cloud quality, bounding box layout and instance features. 
We conduct experiments across datasets on different adaptation scenarios including synthetic-to-real adaptation, point cloud quality adaptation, layout adaptation and instance feature adaptation, analyzing the impact of different domain gaps on 3D object detectors. 
We also introduce several approaches to improve adaptation performances, providing baselines for domain adaptive indoor 3D object detection, hoping that future works may propose detectors with stronger generalization ability across domains.

## Indoor 3D object detection datasets

We provide unified format of existing indoor 3D object detection datasets **ScanNet**, **SUN RGB-D**, **3D Front**
as well as our newly proposed large-scale datasets **ProcTHOR-OD** and **ProcFront**.
Different datasets exhibit different distribution of style, point cloud quality, layout and instance features.

We provide the unified format of these datasets in the following figures, including **the same density**(20,000 points per scene),
**the unified coordinates**(xyz coordinate axis in the figures).

### Existing Datasets

**ScanNet**

<table>
  <tr>
    <td><img src="figures/scannet/01.png" width="100%"></td>
    <td><img src="figures/scannet/02.png" width="100%"></td>
    <td><img src="figures/scannet/03.png" width="100%"></td>
    <td><img src="figures/scannet/04.png" width="100%"></td>
  </tr>
</table>

**SUN RGB-D**

<table>
  <tr>
    <td><img src="figures/sunrgbd/01.png" width="100%"></td>
    <td><img src="figures/sunrgbd/02.png" width="100%"></td>
    <td><img src="figures/sunrgbd/03.png" width="100%"></td>
    <td><img src="figures/sunrgbd/04.png" width="100%"></td>
  </tr>
</table>

**3D Front**

<table>
  <tr>
    <td><img src="figures/3dfront/01.png" width="100%"></td>
    <td><img src="figures/3dfront/02.png" width="100%"></td>
    <td><img src="figures/3dfront/03.png" width="100%"></td>
    <td><img src="figures/3dfront/04.png" width="100%"></td>
  </tr>
</table>


### Our proposed Datasets

We open source the generation code of our proposed ProcTHOR-OD and ProcFront dataset at:

[ProcTHOR-OD](https://github.com/JeremyZhao1998/ProcTHOR-OD)

This repository provides the code of generating the ProcTHOR-OD layouts and the method to export 3D mesh files.

**ProcTHOR-OD**

<table>
  <tr>
    <td><img src="figures/procthor/01.png" width="100%"></td>
    <td><img src="figures/procthor/02.png" width="100%"></td>
    <td><img src="figures/procthor/03.png" width="100%"></td>
    <td><img src="figures/procthor/04.png" width="100%"></td>
  </tr>
</table>

**ProcFront**

<table>
  <tr>
    <td><img src="figures/procfront/01.png" width="100%"></td>
    <td><img src="figures/procfront/02.png" width="100%"></td>
    <td><img src="figures/procfront/03.png" width="100%"></td>
    <td><img src="figures/procfront/04.png" width="100%"></td>
  </tr>
</table>
